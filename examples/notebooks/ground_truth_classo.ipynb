{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7b1c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSO (FISTA) solution a* ≈ [0.68306058 0.         1.21779048]\n",
      "[[0.         0.73982169 0.62651876]\n",
      " [0.73982169 0.         0.82976598]\n",
      " [0.62651876 0.82976598 0.        ]]\n",
      "Spiking LCA spike counts over T=3.0s: [ 68   1 121]\n",
      "Spiking LCA rates (Hz): [0.68 0.01 1.21]\n",
      "Tail-window Tλ(u) readout: [0.68403301 0.         1.21031113]\n",
      "Active (by Tλ(u) > 0): [0 2]\n"
     ]
    }
   ],
   "source": [
    "# Pure-Python Spiking LCA (S-LCA) demo\n",
    "# - Minimal, dependency-free (only numpy)\n",
    "# - Implements the paper's S-LCA with unit-area exponential synapses\n",
    "# - Includes a tiny FISTA solver for CLASSO (nonnegative) to verify the target solution\n",
    "#\n",
    "# You can copy this whole cell into a .py file and run it as a script.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def normalize_columns(Phi: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(Phi, axis=0, keepdims=True) + 1e-12\n",
    "    return Phi / norms\n",
    "\n",
    "def classo_fista_nonneg(Phi, s, lam, max_iter=5000, tol=1e-9):\n",
    "    \"\"\"FISTA for: min_{a >= 0} 0.5||s - Phi a||^2 + lam*||a||_1\"\"\"\n",
    "    Phi = normalize_columns(Phi)\n",
    "    PhiT = Phi.T\n",
    "    PhiTPhi = PhiT @ Phi\n",
    "    PhiTs = PhiT @ s\n",
    "    L = np.linalg.eigvalsh(PhiTPhi).max() + 1e-12  # Lipschitz constant\n",
    "    tstep = 1.0 / L\n",
    "\n",
    "    def prox_nonneg_l1(x, thr):\n",
    "        return np.maximum(0.0, x - thr)\n",
    "\n",
    "    N = Phi.shape[1]\n",
    "    a = np.zeros(N)\n",
    "    y = a.copy()\n",
    "    theta = 1.0\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = PhiTPhi @ y - PhiTs\n",
    "        a_next = prox_nonneg_l1(y - tstep * grad, lam * tstep)\n",
    "        theta_next = 0.5 * (1 + np.sqrt(1 + 4 * theta * theta))\n",
    "        y = a_next + (theta - 1) / theta_next * (a_next - a)\n",
    "        if np.linalg.norm(a_next - a) < tol * (np.linalg.norm(a) + 1e-12):\n",
    "            a = a_next\n",
    "            break\n",
    "        a, theta = a_next, theta_next\n",
    "    return a\n",
    "\n",
    "def spiking_lca(Phi, s, lam=0.1, dt=1e-4, tau_syn=1e-2, T_sec=1000.0,\n",
    "                v_th=1.0, v_reset=0.0, seed=None, return_logs=True,\n",
    "                t0=0.5):\n",
    "    \"\"\"\n",
    "    Spiking LCA with exponential synapses and unit-area scaling.\n",
    "      r[n]   = decay * r[n-1] + sigma_prev\n",
    "      mu     = b - W @ (r / tau_syn)\n",
    "      v     += dt * (mu - lam)\n",
    "      if v >= v_th: spike and reset\n",
    "\n",
    "    Returns:\n",
    "      rates (Hz) from spike counts / T_sec,\n",
    "      spike_counts,\n",
    "      meta dict including (b, W, Phi) and the CLASSO-aligned readout:\n",
    "        - 'Tlam_u': thresholded average current over tail window [t0, T_sec]\n",
    "        - 'u_tail': average soma currents over tail window\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Phi = normalize_columns(Phi)\n",
    "    b   = Phi.T @ s\n",
    "    W   = Phi.T @ Phi\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    print(W)\n",
    "    decay = float(np.exp(-dt / tau_syn))\n",
    "    steps = int(T_sec / dt)\n",
    "    N     = Phi.shape[1]\n",
    "\n",
    "    # States\n",
    "    v = np.zeros(N)\n",
    "    r = np.zeros(N)\n",
    "    sigma_prev = np.zeros(N)\n",
    "    spike_counts = np.zeros(N, dtype=int)\n",
    "\n",
    "    # ----- Added: accumulate ∫ μ dt and snapshot at t0 -----\n",
    "    mu_int = np.zeros(N)        # integral of μ from 0 to current time\n",
    "    mu_int_at_t0 = None         # snapshot of integral at time t0\n",
    "\n",
    "    # Optional logs\n",
    "    if return_logs:\n",
    "        log_every = max(1, int(1e-3 / dt)) # every 1 ms\n",
    "        ts, Vlog, Mlog, Rlog = [], [], [], []\n",
    "\n",
    "    for n in range(steps):\n",
    "        # Exponential filter\n",
    "        r = decay * r + sigma_prev\n",
    "        # Soma current with unit-area scaling\n",
    "        mu = b - (W @ (r / tau_syn))\n",
    "\n",
    "        # ----- Added: accumulate integral and snapshot at t0 -----\n",
    "        mu_int += mu * dt\n",
    "        if (mu_int_at_t0 is None) and ((n + 1) * dt >= t0):\n",
    "            mu_int_at_t0 = mu_int.copy()\n",
    "\n",
    "        # Voltage integration with bias\n",
    "        v += dt * (mu - lam)\n",
    "        # Spikes\n",
    "        sigma = (v >= v_th).astype(float)\n",
    "        if sigma.any():\n",
    "            spike_counts += sigma.astype(int)\n",
    "            v[sigma > 0] = v_reset\n",
    "        sigma_prev = sigma\n",
    "\n",
    "        if return_logs and (n % log_every == 0):\n",
    "            ts.append(n * dt)\n",
    "            Vlog.append(v.copy())\n",
    "            Mlog.append(mu.copy())\n",
    "            Rlog.append(r.copy())\n",
    "\n",
    "    # Default snapshot if t0 is 0 or sim is too short\n",
    "    if mu_int_at_t0 is None:\n",
    "        mu_int_at_t0 = np.zeros_like(mu_int)\n",
    "\n",
    "    # ----- Added: compute tail-window averages and Tλ(u) readout -----\n",
    "    denom = max(T_sec - t0, 1e-12)\n",
    "    u_tail = (mu_int - mu_int_at_t0) / denom\n",
    "    Tlam_u = np.maximum(0.0, u_tail - lam)   # CLASSO-aligned readout\n",
    "\n",
    "    rates = spike_counts / T_sec\n",
    "    meta = {\n",
    "        \"t\": np.array(ts) if return_logs else None,\n",
    "        \"v\": np.array(Vlog) if return_logs else None,\n",
    "        \"mu\": np.array(Mlog) if return_logs else None,\n",
    "        \"r\": np.array(Rlog) if return_logs else None,\n",
    "        \"b\": b, \"W\": W, \"Phi\": Phi,\n",
    "        \"u_tail\": u_tail, \"Tlam_u\": Tlam_u, \"t0\": t0\n",
    "    }\n",
    "    return rates, spike_counts, meta\n",
    "\n",
    "# ------------------- Demo with the paper's 3-neuron example -------------------\n",
    "Phi_demo = np.array([\n",
    "    [0.3313, 0.8148, 0.4364],\n",
    "    [0.8835, 0.3621, 0.2182],\n",
    "    [0.3313, 0.4527, 0.8729],\n",
    "], dtype=float)\n",
    "s_demo = np.array([0.5, 1.0, 1.5], dtype=float)\n",
    "lam_demo = 0.1\n",
    "\n",
    "# 1) Closed-form target via FISTA (nonnegative CLASSO)\n",
    "a_star = classo_fista_nonneg(Phi_demo, s_demo, lam_demo)\n",
    "print(\"CLASSO (FISTA) solution a* ≈\", a_star)\n",
    "\n",
    "# 2) Spiking LCA run\n",
    "rates, counts, meta = spiking_lca(\n",
    "    Phi_demo, s_demo, lam=lam_demo,\n",
    "    dt=1e-3, tau_syn=1, T_sec=100,\n",
    "    t0=1,  # tail window start (seconds); tune 5–20% of T_sec\n",
    "    return_logs=True\n",
    ")\n",
    "\n",
    "print(\"Spiking LCA spike counts over T=3.0s:\", counts)\n",
    "print(\"Spiking LCA rates (Hz):\", rates)\n",
    "print(\"Tail-window Tλ(u) readout:\", meta[\"Tlam_u\"])\n",
    "print(\"Active (by Tλ(u) > 0):\", np.where(meta[\"Tlam_u\"] > 0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e916909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5352b6c0",
   "metadata": {},
   "source": [
    "# Systematic Comparison: Ground Truth vs Fugu Implementation\n",
    "\n",
    "## Key Differences to Investigate:\n",
    "1. **Normalization**: Are we double-normalizing Phi?\n",
    "2. **Edge weights**: Are lateral inhibition weights correct?\n",
    "3. **Integration method**: Different membrane potential updates?\n",
    "4. **Readout method**: Different ways to compute final sparse codes?\n",
    "5. **Parameters**: Different dt, tau, lambda values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d137c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NORMALIZATION ANALYSIS ===\n",
      "Ground truth normalized Phi:\n",
      "[[0.33128482 0.81481925 0.43639768]\n",
      " [0.88345953 0.36210856 0.21819884]\n",
      " [0.33128482 0.4527107  0.87289537]]\n",
      "Column norms: [1. 1. 1.]\n",
      "\n",
      "Fugu backend Phi (after compile):\n",
      "(This will show if there's double normalization)\n",
      "\n",
      "Ground truth b = Phi^T @ s:\n",
      "[1.54602917 1.44858423 1.74574074]\n"
     ]
    }
   ],
   "source": [
    "# 1. NORMALIZATION CHECK\n",
    "print(\"=== NORMALIZATION ANALYSIS ===\")\n",
    "\n",
    "# Ground truth normalization\n",
    "Phi_raw = np.array([\n",
    "    [0.3313, 0.8148, 0.4364],\n",
    "    [0.8835, 0.3621, 0.2182],\n",
    "    [0.3313, 0.4527, 0.8729],\n",
    "], dtype=float)\n",
    "\n",
    "def normalize_columns_gt(Phi):\n",
    "    norms = np.linalg.norm(Phi, axis=0, keepdims=True) + 1e-12\n",
    "    return Phi / norms\n",
    "\n",
    "Phi_gt_normalized = normalize_columns_gt(Phi_raw)\n",
    "print(\"Ground truth normalized Phi:\")\n",
    "print(Phi_gt_normalized)\n",
    "print(\"Column norms:\", np.linalg.norm(Phi_gt_normalized, axis=0))\n",
    "\n",
    "# Check what the Fugu backend received\n",
    "print(\"\\nFugu backend Phi (after compile):\")\n",
    "print(\"(This will show if there's double normalization)\")\n",
    "\n",
    "# We need to access the actual compiled Phi from the ground truth demo first\n",
    "print(\"\\nGround truth b = Phi^T @ s:\")\n",
    "b_gt = Phi_gt_normalized.T @ np.array([0.5, 1.0, 1.5])\n",
    "print(b_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bf590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== FUGU BACKEND ANALYSIS ===\n",
      "Fugu backend.Phi shape: (3, 3)\n",
      "Fugu backend.Phi column norms: [1. 1. 1.]\n",
      "Fugu backend.b: [1.54602917 1.44858423 1.74574074]\n",
      "Fugu backend.W shape: (3, 3)\n",
      "Fugu backend.W[0,1]: 0.7398216888397283\n",
      "Fugu backend.tau: 1.0\n",
      "Fugu backend.dt: 0.001\n"
     ]
    }
   ],
   "source": [
    "# 2. COMPARE FUGU BACKEND PARAMETERS\n",
    "print(\"\\\\n=== FUGU BACKEND ANALYSIS ===\")\n",
    "\n",
    "# Navigate to lca_dev notebook and get backend state\n",
    "import sys\n",
    "sys.path.append('/Users/kamerongano/Documents/GitHub/Fugu_dev')\n",
    "\n",
    "# Import the results from the lca_dev notebook (we'll reconstruct)\n",
    "# Using the exact same parameters\n",
    "from fugu.backends import slca_Backend\n",
    "from fugu.bricks import LCABrick \n",
    "from fugu import Scaffold\n",
    "\n",
    "Phi_demo = np.array([\n",
    "    [0.3313, 0.8148, 0.4364],\n",
    "    [0.8835, 0.3621, 0.2182],\n",
    "    [0.3313, 0.4527, 0.8729],\n",
    "], dtype=float)\n",
    "s_demo = np.array([0.5, 1.0, 1.5], dtype=float)\n",
    "lam_demo = 0.1\n",
    "\n",
    "scaffold = Scaffold()\n",
    "scaffold.add_brick(LCABrick(Phi=Phi_demo), output=True)\n",
    "scaffold.lay_bricks()\n",
    "\n",
    "backend = slca_Backend()\n",
    "backend.compile(\n",
    "    scaffold=scaffold,\n",
    "    compile_args={\n",
    "        'y': s_demo,\n",
    "        'Phi': Phi_demo,\n",
    "        'lam': lam_demo,\n",
    "        'dt': 1e-3,        # Ground truth uses dt=1e-3\n",
    "        'tau_syn': 1,      # Ground truth uses tau=1  \n",
    "        'T_steps': 1000,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Fugu backend.Phi shape:\", backend.Phi.shape)\n",
    "print(\"Fugu backend.Phi column norms:\", np.linalg.norm(backend.Phi, axis=0))\n",
    "print(\"Fugu backend.b:\", backend.b)\n",
    "print(\"Fugu backend.W shape:\", backend.W.shape)\n",
    "print(\"Fugu backend.W[0,1]:\", backend.W[0,1])\n",
    "print(\"Fugu backend.tau:\", backend.tau)\n",
    "print(\"Fugu backend.dt:\", backend.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf84b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== INTEGRATION METHOD COMPARISON ===\n",
      "Ground truth method:\n",
      "  r[n] = decay * r[n-1] + sigma_prev\n",
      "  mu = b - W @ (r / tau_syn)  # Note: r/tau scaling\n",
      "  v += dt * (mu - lam)\n",
      "\\nFugu backend method (let's check the code):\n",
      "\\nManual step-by-step comparison:\n",
      "Initial states - GT r: [0. 0. 0.], Fugu inhibition: [0. 0. 0.]\n",
      "Step 1 - GT mu: [1.54602917 1.44858423 1.74574074]\n",
      "Step 1 - Fugu mu: [1.54602917 1.44858423 1.74574074]\n",
      "Should be identical: True\n",
      "GT voltage update: dv = dt*(mu-lam) = [0.00144603 0.00134858 0.00164574]\n",
      "Expected new voltages: [0.00144603 0.00134858 0.00164574]\n",
      "\\n=== CHECKING FUGU VOLTAGE UPDATE METHOD ===\n",
      "Let's examine what the Fugu slca_step actually does to voltages...\n"
     ]
    }
   ],
   "source": [
    "# 3. SIDE-BY-SIDE INTEGRATION COMPARISON\n",
    "print(\"\\\\n=== INTEGRATION METHOD COMPARISON ===\")\n",
    "\n",
    "# Ground truth method (from the notebook above)\n",
    "print(\"Ground truth method:\")\n",
    "print(\"  r[n] = decay * r[n-1] + sigma_prev\")\n",
    "print(\"  mu = b - W @ (r / tau_syn)  # Note: r/tau scaling\")\n",
    "print(\"  v += dt * (mu - lam)\")\n",
    "\n",
    "print(\"\\\\nFugu backend method (let's check the code):\")\n",
    "\n",
    "# Let's run a few steps manually to compare\n",
    "print(\"\\\\nManual step-by-step comparison:\")\n",
    "\n",
    "# Initialize states for both methods\n",
    "# Ground truth states\n",
    "decay_gt = np.exp(-1e-3 / 1.0)  # exp(-dt/tau)\n",
    "r_gt = np.zeros(3)\n",
    "v_gt = np.zeros(3) \n",
    "sigma_prev_gt = np.zeros(3)\n",
    "\n",
    "# Fugu states (from backend)\n",
    "backend.inhibition[:] = 0.0\n",
    "backend.soma_current[:] = 0.0  \n",
    "backend.spikes_prev[:] = 0.0\n",
    "# Initialize neurons to 0\n",
    "for name, n in backend.nn.nrns.items():\n",
    "    if \"neuron_\" in name:\n",
    "        n.v = 0.0\n",
    "\n",
    "print(f\"Initial states - GT r: {r_gt}, Fugu inhibition: {backend.inhibition}\")\n",
    "\n",
    "# Step 1: No spikes initially, so mu = b in both cases\n",
    "mu_gt_step1 = backend.b - backend.W @ (r_gt / backend.tau)  # GT method\n",
    "backend.soma_current = backend.b - backend.W @ backend.inhibition  # Fugu method\n",
    "\n",
    "print(f\"Step 1 - GT mu: {mu_gt_step1}\")\n",
    "print(f\"Step 1 - Fugu mu: {backend.soma_current}\")\n",
    "print(f\"Should be identical: {np.allclose(mu_gt_step1, backend.soma_current)}\")\n",
    "\n",
    "# Check voltage updates\n",
    "v_update_gt = backend.dt * (mu_gt_step1 - backend.lam)\n",
    "print(f\"GT voltage update: dv = dt*(mu-lam) = {v_update_gt}\")\n",
    "print(f\"Expected new voltages: {v_gt + v_update_gt}\")\n",
    "\n",
    "# The issue might be in how Fugu applies the voltage update!\n",
    "print(\"\\\\n=== CHECKING FUGU VOLTAGE UPDATE METHOD ===\")\n",
    "print(\"Let's examine what the Fugu slca_step actually does to voltages...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc74b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== TESTING CORRECTED FUGU IMPLEMENTATION ===\n",
      "\\n=== COMPARISON RESULTS ===\n",
      "Ground Truth CLASSO: [0.683, 0.0, 1.218]\n",
      "Ground Truth S-LCA:  [0.684, 0.0, 1.210]\n",
      "Fugu Corrected:      [0.68402167 0.         1.21029842]\n",
      "\n",
      "Ground Truth active: [0, 2]\n",
      "Fugu active:         [0 2]\n",
      "\n",
      "Reconstruction error:\n",
      "  Ground Truth: 0.359666\n",
      "  Fugu:         0.359571\n",
      "\n",
      "Success? True\n"
     ]
    }
   ],
   "source": [
    "# 4. TEST CORRECTED FUGU IMPLEMENTATION\n",
    "print(\"\\\\n=== TESTING CORRECTED FUGU IMPLEMENTATION ===\")\n",
    "\n",
    "# Force reload the fixed backend\n",
    "import importlib\n",
    "if 'fugu.backends.slca_backend' in sys.modules:\n",
    "    importlib.reload(sys.modules['fugu.backends.slca_backend'])\n",
    "from fugu.backends import slca_Backend\n",
    "\n",
    "# Create corrected backend with EXACT same parameters as ground truth\n",
    "backend_corrected = slca_Backend()\n",
    "backend_corrected.compile(\n",
    "    scaffold=scaffold,\n",
    "    compile_args={\n",
    "        'y': s_demo,\n",
    "        'Phi': Phi_demo,\n",
    "        'lam': 0.1,\n",
    "        'dt': 1e-3,      # Same as ground truth\n",
    "        'tau_syn': 1.0,  # Same as ground truth  \n",
    "        'T_steps': 100000,  # Same duration as ground truth\n",
    "        't0_steps': 1000,   # Tail window start\n",
    "    }\n",
    ")\n",
    "\n",
    "result_corrected = backend_corrected.run()\n",
    "\n",
    "print(\"\\\\n=== COMPARISON RESULTS ===\")\n",
    "print(f\"Ground Truth CLASSO: [0.683, 0.0, 1.218]\")\n",
    "print(f\"Ground Truth S-LCA:  [0.684, 0.0, 1.210]\") \n",
    "print(f\"Fugu Corrected:      {result_corrected['a_tail']}\")\n",
    "print(f\"\")\n",
    "print(f\"Ground Truth active: [0, 2]\")\n",
    "print(f\"Fugu active:         {np.where(result_corrected['a_tail'] > 0.01)[0]}\")\n",
    "print(f\"\")\n",
    "print(f\"Reconstruction error:\")\n",
    "print(f\"  Ground Truth: {np.linalg.norm(Phi_gt_normalized @ np.array([0.684, 0.0, 1.210]) - s_demo):.6f}\")\n",
    "print(f\"  Fugu:         {np.linalg.norm(result_corrected['x_hat'] - s_demo):.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"Success? {np.allclose(result_corrected['a_tail'], [0.684, 0.0, 1.210], atol=0.1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fugudev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
